---
title: "RMINC Parallelism"
author: "Chris Hammill"
date: "October 20, 2016"
output: html_document
---

The vast majority of problems tackled by RMINC as of 2016 are considered
["embarassingly parallel"](https://en.wikipedia.org/wiki/Embarrassingly_parallel) this means that
the problem can be broken into smaller pieces that can be solved completely independently. Take
for example the standard massively-univariate approach to analyzing neuroimages. Each image
is composed of voxels, and a separate model is computed at each voxel. These models are computed
without any dependency on the models being computed at other voxels. Since there is no
interdependency it is easy to split the voxels up between multiple cores on a computer, or 
even between different computers and combine the results after. 

## A Basic Example 

Imagine for example there are some simple images that are 10 x 10 x 10 voxels. 

```{r}
library(RMINC)
library(dplyr)
getRMINCTestData("./")

image_frame <- read.csv("test_data_set.csv", stringsAsFactors = FALSE)

image_frame$jacobians_fixed_2[1] %>%
  mincGetVolume %>%
  mincArray %>%
  mincPlotSliceSeries(mfrow = c(2,2))
```

Now to see if the values at each voxel depend on some variable, body weight for example, we can fit a linear model. Since 
the models are independent we can divide the problem up. I'll use four cores on my computer, each one will fit 250 of the 1000
models, and then the results can be stuck back together.

```{r}
model <- mincLm(jacobians_fixed_2 ~ Body.Weight, data = image_frame, parallel = c("local", 4))

image_frame$jacobians_fixed_2[1] %>%
  mincGetVolume %>%
  mincArray %>%
  mincPlotSliceSeries(statistic = mincArray(model, "tvalue-Body.Weight")
                      , symmetric = TRUE
                      , low = 2, high = 5
                      , legend = "t-value", locator = FALSE
                      , mfrow = c(2,2))
```

Because the job is divided between 4 cores the computation goes much faster. There is some overhead to splitting
and recombining the job, so you won't get a 4x speedup, but it will be faster. The argument `parallel = c("local", 4)`
says to run the jobs on 4 cores of the local machine. 

This system is even more powerful if you have a cluster at your disposal. You can divide the problem in to potentially
100s of peices that can be solved independently.

### Parallel Functions

As of Oct. 20th, 2016, The following functions support the parallel option as used above:

1. mincLm
2. pMincApply
3. mincLmer
4. anatGetAll
5. anatLmer
6. vertexApply
7. vertexLmer

Each of these can run sequentially, locally (multicore), or on a cluster. 

## RMINC on a Cluster

Using RMINC on a cluster is now relatively simple. Thanks to the wonderful 
["BatchJobs"](https://github.com/tudo-r/BatchJobs/blob/master/README.md)
RMINC can be configured to run on Torque, SGE, Slurm, ad-hoc SSH clusters and more. We and our collaborators typically 
use Torque and SGE but using these other systems is possible with a little configuration effort. 

### Quick Start

To setup RMINC to run on your cluster you can do one of the following:

  1. Use RMINC's `configureMincParallel` with either  "pbs", "sge", or a custom configuration file.
     Both the PBS (torque) and SGE options will load up default configurations that work well for our
     system. Your mileage may vary.
  2. Use BatchJobs `loadConfig` or `setConfig`, with a custom configuration.
  
For example:

```{r}
configureMincParallel("sge")

qmodel <- mincLm(jacobians_fixed_2 ~ Body.Weight, data = image_frame, parallel = c("sge", 6))

image_frame$jacobians_fixed_2[1] %>%
  mincGetVolume %>%
  mincArray %>%
  mincPlotSliceSeries(statistic = mincArray(qmodel, "tvalue-Body.Weight")
                      , symmetric = TRUE
                      , low = 2, high = 5
                      , legend = "t-value", locator = FALSE
                      , mfrow = c(2,2))

```

As of RMINC 1.4.3 there is no configuration necessary within parallel function calls other than to specify the
number of batches. The argument `parallel = c("sge", 6)` is splitting the job into 6 peices. The `"sge"` portion
is checked to see if it is "local" or "snowfall", otherwise it is ignored, relying instead on BatchJobs configuration
to control execution. This contrasts from earlier versions of RMINC.


### Site Configuration

To make it so you do not need to configure BatchJobs each time you open an R session you can do one of
the following:

  1. Use BatchJobs [configuration files](https://github.com/tudo-r/BatchJobs/wiki/Configuration),
     If you are configuring for your site this will likely involve editing the configuration files
     in the BatchJobs package itself.
  2. Use the Rprofile mechanisms (see `?Startup`) to load the following: 
     ```
     setHook(packageEvent("RMINC", "attach"),
           function(...) 
              BatchJobs::loadConfig(system.file("parallel/pbs_BatchJobs.R", package = "RMINC")))
     ```
     For our setup, I organized a module that sets the `R_PROFILE` environment variable (in addition
     to the package libraries) that points to a file containing the above code. This instructs
     R to load the pbs configuration file when RMINC is attached.

I recommend option two.

The default PBS configuration file looks like:

```{r, eval = FALSE}
cluster.functions <- makeClusterFunctionsTorque(system.file("parallel/pbs_script.tmpl", package = "RMINC"))
default.resources <-
  list(nodes = 1,
       vmem = "8G",
       walltime = "01:00:00")
```


It is R code that sets the `cluster.functions` and `default.resources` for BatchJobs. The function 
`makeClusterFunctionsTorque` finds a template file that it fills in whenever a new job is created. The
exampled PBS template looks like:

```
## Job Resource Interface Definition
##
## nodes [integer|character]:  Argument to PBS -l nodes, may be an integer number of nodes, or a character vector
##                             of a colon separated list of node arguments e.g "1:ppn=8" for a scinet node
## walltime   [character(1)]:  Walltime for this job e.g. "01:00:00" for 1 hour
## vmem       [character(1)]:  Memory for each job e.g. "2G" for 2 gigabytes
## extra         [character]:  Extra lines to be added to the job script before calling R  
## 'walltime' and 'memory' settings automatically determine the correct queue, you don't have to
## select the queue yourself.
## Default resources can be set in your .BatchJobs.R by defining the variable
## 'default.resources' as a named list.


#PBS -N <%= job.name %>
## merge standard error and output
#PBS -j oe
## direct streams to our logfile
#PBS -o <%= log.file %>

#PBS -l nodes=<%= resources$nodes %>,walltime=<%= resources$walltime %>,vmem=<%= resources$vmem %>
## remove this line if your cluster does not support arrayjobs
## Disable array jobs: #PBS -t 1-<%= arrayjobs %>
#PBS -V  
## Run R:
## we merge R output with stdout from PBS, which gets then logged via -o option
<% if(!is.null(resources$extra)) cat(resources$extra, sep = "\n") %>
R CMD BATCH --no-save --no-restore "<%= rscript %>" /dev/stdout
```

BatchJobs handles the job.name, log.file, and rscript portions of the template. Our default.resources
configuration provides the nodes, walltime, and vmem.

If you do not use PBS or SGE, you can find example configurations on 
[BatchJobs' github](https://github.com/tudo-r/BatchJobs/tree/master/examples) 

If in the middle of a session you need to change you configuration you can always use
`BatchJobs::setConfig`, `BatchJobs::loadConfig`, and `RMINC::configureMincParallel`. 
For example to ask for twice as much walltime you can run:

```{r, eval = FALSE}
setConfig(
  list(default.resources = 
         list(nodes = 1, vmem = "8G", walltime = "02:00:00")))
```
```{r, include=FALSE}
setConfig(
  list(default.resources = 
         list(nodes = 1, vmem = "8G", walltime = "02:00:00", extras = "-l q=defdev.q")))
```

I know the nested lists are cumbersome, but ideally reconfiguring mid-session should be rare.

It is also important to note that with RMINC 1.4.3 nested parallelism (as is required for SciNet)
has been discontinued. It introduced unnecessary fragility into the code. If enough users request
this be supported it will be re-added to a future release.
